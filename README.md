
# LLM Answer Engine (Python Version)

A modular, scalable LLM-based answer generation engine that:
- Rephrases search queries intelligently
- Searches the web (Brave / Serper)
- Scrapes webpages for context
- Applies RAG (Retrieval Augmented Generation)
- Generates answers and follow-up questions

Built purely in **Python + FastAPI** with production-grade architecture.

---

## ğŸš€ Features

- âœ… OpenAI / Groq model switching
- âœ… Web search (Serper/Brave API based)
- âœ… Web scraping & HTML parsing
- âœ… FAISS-based RAG (Retrieval-Augmented Generation)
- âœ… Redis Caching and Rate Limiting
- âœ… Modularized, extensible services
- âœ… Async, fast, and efficient

---

## ğŸ›  Tech Stack

- FastAPI
- LangChain
- OpenAI Python SDK
- Redis (async)
- FAISS (CPU)
- BeautifulSoup (scraping)
- Docker (for Redis)
- Pydantic v2

---

## ğŸ“‚ Folder Structure

```bash
app/
â”œâ”€â”€ api/                 # FastAPI routes
â”‚   â””â”€â”€ answer.py
â”œâ”€â”€ core/                # Config and Logger
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ models/              # Pydantic schemas
â”‚   â””â”€â”€ schemas.py
â”œâ”€â”€ services/            # All core services
â”‚   â”œâ”€â”€ answer_service.py
â”‚   â”œâ”€â”€ llm.py
â”‚   â”œâ”€â”€ rag.py
â”‚   â”œâ”€â”€ scraper.py
â”‚   â”œâ”€â”€ search.py
â”‚   â”œâ”€â”€ search_selector.py
â”‚   â”œâ”€â”€ utils.py
â”œâ”€â”€ cache.py             # Redis connection & operations
â”œâ”€â”€ main.py              # FastAPI entrypoint
```

---

## âš™ï¸ Prerequisites

- Python 3.10+
- Docker (for Redis)
- OpenAI API key (or Groq key)
- Brave API key (optional if using Serper)

---

## ğŸ›  Installation Steps

```bash
# 1. Clone the repo
git clone https://github.com/your-username/llm-answer-engine.git
cd llm-answer-engine

# 2. Create a virtual environment
python3 -m venv venv
source venv/bin/activate

# 3. Install requirements
pip install -r requirements.txt

# 4. Create your .env file
cp .env.example .env
# Fill your keys inside
```

---

## ğŸ§¹ Running the Application

### Run Redis using Docker:

```bash
docker run -d -p 6379:6379 redis
```

### Run FastAPI server:

```bash
uvicorn app.main:app --reload
```

Visit: http://localhost:8000/docs for Swagger UI.

---

## ğŸ“š API Endpoint Documentation

### POST `/answer`

**Request Body:**

```json
{
  "message": "Best places to visit in Paris",
  "return_sources": true,
  "return_follow_up_questions": true,
  "embed_sources_in_llm_response": false
}
```

**Response:**

```json
{
  "answer": "Some top places in Paris are Eiffel Tower...",
  "sources": [
    {"title": "Tripadvisor - Paris", "link": "https://www.tripadvisor.com/..."},
    ...
  ],
  "follow_up_questions": [
    "What are top restaurants in Paris?",
    "Best time to visit Paris?"
  ]
}
```

---

## ğŸ§© Environment Variables Example (.env)

```env
# LLM
llm_provider=openai
openai_api_key=your_openai_key
groq_api_key=your_groq_key

# Search
search_provider=serper
serper_api_key=your_serper_key
brave_api_key=your_brave_key

# Settings
answer_model=gpt-3.5-turbo
followup_model=gpt-3.5-turbo
requests_per_minute=30

# Redis
redis_host=localhost
redis_port=6379
redis_db=0
```

---

## ğŸš€ Future Enhancements

- Add support for Azure OpenAI
- Add Anthropic Claude models
- Add Weaviate / Chroma vectorstores
- Add WebSocket streaming

---

## ğŸ“œ License

MIT License.

---

## ğŸ¤ Credits

Built with â¤ï¸ by Rahul Pandey.

---
